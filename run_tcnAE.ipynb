{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tcnAutoencoder import TCNAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 738034 unique order book shapshots from ./data/hft_data/AAPL/AAPL_2024-03-01_34200000_57600000_message_10.csv\n",
      "Read 1923409 unique order book shapshots from ./data/hft_data/AAPL/AAPL_2024-03-04_34200000_57600000_message_10.csv\n",
      "Read 2108353 unique order book shapshots from ./data/hft_data/AAPL/AAPL_2024-03-05_34200000_57600000_message_10.csv\n",
      "Read 2364167 unique order book shapshots from ./data/hft_data/AAPL/AAPL_2024-03-06_34200000_57600000_message_10.csv\n",
      "Read 1732063 unique order book shapshots from ./data/hft_data/AAPL/AAPL_2024-03-07_34200000_57600000_message_10.csv\n",
      "Read 3123866 unique order book shapshots from ./data/hft_data/AAPL/AAPL_2024-03-08_34200000_57600000_message_10.csv\n"
     ]
    }
   ],
   "source": [
    "# %load load_data.py\n",
    "import pandas as pd\n",
    "import glob, os, re\n",
    "\n",
    "\n",
    "# Read the data only once.  It's big!\n",
    "csv_files = glob.glob(os.path.join(\".\", \"data\", \"hft_data\", \"*\", \"*_message_*.csv\"))\n",
    "date_str = re.compile(r'_(\\d{4}-\\d{2}-\\d{2})_')\n",
    "stock_str = re.compile(r'([A-Z]+)_\\d{4}-\\d{2}-\\d{2}_')\n",
    "\n",
    "df_list = []\n",
    "day_list = []\n",
    "sym_list = []\n",
    "\n",
    "for csv_file in sorted(csv_files):\n",
    "    date = date_str.search(csv_file)\n",
    "    date = date.group(1)\n",
    "    day_list.append(date)\n",
    "\n",
    "    symbol = stock_str.search(csv_file)\n",
    "    symbol = symbol.group(1)\n",
    "    sym_list.append(symbol)\n",
    "\n",
    "    # Find the order book file that matches this message file.\n",
    "    book_file = csv_file.replace(\"message\", \"orderbook\")\n",
    "\n",
    "    # Read the message file and index by timestamp.\n",
    "    df = pd.read_csv(csv_file, names=['Time','EventType','OrderID','Size','Price','Direction'])\n",
    "    df['Time'] = pd.to_datetime(date) + pd.to_timedelta(df['Time'], unit='s')\n",
    "\n",
    "    # Read the order book file and merge it with the messages.\n",
    "    names = [f\"{x}{i}\" for i in range(1,11) for x in [\"AP\",\"AS\",\"BP\",\"BS\"]]\n",
    "    df = df.join(pd.read_csv(book_file, names=names), how='inner')\n",
    "    df = df.set_index(['Time'])\n",
    "\n",
    "    BBID_COL = df.columns.get_loc(\"BP1\")\n",
    "    BASK_COL = df.columns.get_loc(\"AP1\")\n",
    "\n",
    "    print (f\"Read {df.shape[0]} unique order book shapshots from {csv_file}\")\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "days = len(day_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:  (234000, 2)\n",
      "original shape:  (234000, 2)\n",
      "original shape:  (234000, 2)\n",
      "original shape:  (234000, 2)\n",
      "original shape:  (234000, 2)\n",
      "original shape:  (234000, 2)\n"
     ]
    }
   ],
   "source": [
    "def prep_data(df) -> pd.DataFrame:\n",
    "    df = df[['Price', 'Size']]\n",
    "    df.head()\n",
    "\n",
    "    # sample every 100ms, and the size would be the sum of the size in that 100ms. \n",
    "    # Price would be the average price in that 100ms.\n",
    "    df = df.resample('100ms').agg({'Price': 'mean', 'Size': 'sum'})\n",
    "\n",
    "    # Check for NaN values\n",
    "\n",
    "    # forwardfill all NaN values in the data\n",
    "    df = df.ffill()\n",
    "\n",
    "    # normalize the data with mean and std\n",
    "    mean = df['Price'].mean()\n",
    "    std = df['Price'].std()\n",
    "    df['Price'] = (df['Price'] - mean) / std\n",
    "\n",
    "    mean = df['Size'].mean()\n",
    "    std = df['Size'].std()\n",
    "    df['Size'] = (df['Size'] - mean) / std\n",
    "\n",
    "    print(\"original shape: \", df.shape)\n",
    "\n",
    "    df = df.values\n",
    "    # Create a tensor for every 30 minutes of data\n",
    "    tensors = []\n",
    "    for i in range(0, len(df), 18000):\n",
    "        if i + 18000 < len(df):\n",
    "            # flip the first and second dimension, so that the shape is (batch_size, channel, sequence_length)\n",
    "            tensors.append(torch.tensor(df[i:i+18000], dtype = torch.float32).unsqueeze(0))\n",
    "        else:\n",
    "            tensors.append(torch.tensor(df[i:], dtype = torch.float32).unsqueeze(0))\n",
    "\n",
    "    return tensors\n",
    "\n",
    "    # Create the final torch tensor, every 1 hour is a sequence\n",
    "\n",
    "tensors_list = []\n",
    "for df in df_list: \n",
    "    tensors_list.extend(prep_data(df_list[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filp the first and second dimension, so that the shape is (batch_size, channel, sequence_length)\n",
    "tensors_list = [tensor.permute(0, 2, 1) for tensor in tensors_list]\n",
    "\n",
    "# [print(tensor.shape) for tensor in tensors_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device mps\n"
     ]
    }
   ],
   "source": [
    "# Start the training process with tensors.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "model = TCNAutoencoder(input_dim=(2, 18000)).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# Randomly sample 0.2 of the data from the batch for testing, excluding them for traning.\n",
    "tensors = tensors_list\n",
    "tensors_train = tensors[:int(len(tensors) * 0.8)]\n",
    "tensors_test = tensors[int(len(tensors) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chan/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1041.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9999473690986633\n",
      "Time taken (seconds): 3.127906084060669\n",
      "Epoch: 1, Loss: 0.9984509944915771\n",
      "Time taken (seconds): 1.5309269428253174\n",
      "Epoch: 2, Loss: 0.9972053170204163\n",
      "Time taken (seconds): 1.5261731147766113\n",
      "Epoch: 3, Loss: 0.9961170554161072\n",
      "Time taken (seconds): 1.5162310600280762\n",
      "Epoch: 4, Loss: 0.9942377209663391\n",
      "Time taken (seconds): 1.5144472122192383\n",
      "Epoch: 5, Loss: 0.9917955994606018\n",
      "Time taken (seconds): 1.5248522758483887\n",
      "Epoch: 6, Loss: 0.9887092709541321\n",
      "Time taken (seconds): 1.5153429508209229\n",
      "Epoch: 7, Loss: 0.9847173094749451\n",
      "Time taken (seconds): 1.5102591514587402\n",
      "Epoch: 8, Loss: 0.9796700477600098\n",
      "Time taken (seconds): 1.524878978729248\n",
      "Epoch: 9, Loss: 0.9734163284301758\n",
      "Time taken (seconds): 1.5150129795074463\n",
      "Epoch: 10, Loss: 0.965355634689331\n",
      "Time taken (seconds): 1.5175821781158447\n",
      "Epoch: 11, Loss: 0.9553149938583374\n",
      "Time taken (seconds): 1.5258960723876953\n",
      "Epoch: 12, Loss: 0.9430125951766968\n",
      "Time taken (seconds): 1.5073580741882324\n",
      "Epoch: 13, Loss: 0.9285323619842529\n",
      "Time taken (seconds): 1.5159060955047607\n",
      "Epoch: 14, Loss: 0.9108182787895203\n",
      "Time taken (seconds): 1.525557279586792\n",
      "Epoch: 15, Loss: 0.8893331289291382\n",
      "Time taken (seconds): 1.5192511081695557\n",
      "Epoch: 16, Loss: 0.8631995916366577\n",
      "Time taken (seconds): 1.5137999057769775\n",
      "Epoch: 17, Loss: 0.8314358592033386\n",
      "Time taken (seconds): 1.5143840312957764\n",
      "Epoch: 18, Loss: 0.7925167083740234\n",
      "Time taken (seconds): 1.5169248580932617\n",
      "Epoch: 19, Loss: 0.747024416923523\n",
      "Time taken (seconds): 1.517568826675415\n",
      "Epoch: 20, Loss: 0.6980264186859131\n",
      "Time taken (seconds): 1.515132188796997\n",
      "Epoch: 21, Loss: 0.6598294973373413\n",
      "Time taken (seconds): 1.5174870491027832\n",
      "Epoch: 22, Loss: 0.66309654712677\n",
      "Time taken (seconds): 1.5203680992126465\n",
      "Epoch: 23, Loss: 0.6582995057106018\n",
      "Time taken (seconds): 1.5092182159423828\n",
      "Epoch: 24, Loss: 0.617068350315094\n",
      "Time taken (seconds): 1.5083661079406738\n",
      "Epoch: 25, Loss: 0.5782248377799988\n",
      "Time taken (seconds): 1.5110061168670654\n",
      "Epoch: 26, Loss: 0.5631806254386902\n",
      "Time taken (seconds): 1.543255090713501\n",
      "Epoch: 27, Loss: 0.568734884262085\n",
      "Time taken (seconds): 1.53849196434021\n",
      "Epoch: 28, Loss: 0.5830104351043701\n",
      "Time taken (seconds): 1.5249507427215576\n",
      "Epoch: 29, Loss: 0.584315299987793\n",
      "Time taken (seconds): 1.533614158630371\n",
      "Epoch: 30, Loss: 0.5671461820602417\n",
      "Time taken (seconds): 1.5176599025726318\n",
      "Epoch: 31, Loss: 0.5492652654647827\n",
      "Time taken (seconds): 1.5175609588623047\n",
      "Epoch: 32, Loss: 0.5412739515304565\n",
      "Time taken (seconds): 1.509984016418457\n",
      "Epoch: 33, Loss: 0.5420082807540894\n",
      "Time taken (seconds): 1.5095040798187256\n",
      "Epoch: 34, Loss: 0.5456324219703674\n",
      "Time taken (seconds): 1.5648038387298584\n",
      "Epoch: 35, Loss: 0.5467346906661987\n",
      "Time taken (seconds): 1.5500481128692627\n",
      "Epoch: 36, Loss: 0.5445797443389893\n",
      "Time taken (seconds): 1.515887975692749\n",
      "Epoch: 37, Loss: 0.5413215756416321\n",
      "Time taken (seconds): 1.507349967956543\n",
      "Epoch: 38, Loss: 0.5387244820594788\n",
      "Time taken (seconds): 1.512765884399414\n",
      "Epoch: 39, Loss: 0.5370187163352966\n",
      "Time taken (seconds): 1.5245709419250488\n",
      "Epoch: 40, Loss: 0.5356926918029785\n",
      "Time taken (seconds): 1.5131442546844482\n",
      "Epoch: 41, Loss: 0.5344038009643555\n",
      "Time taken (seconds): 1.5076589584350586\n",
      "Epoch: 42, Loss: 0.5331981182098389\n",
      "Time taken (seconds): 1.5117440223693848\n",
      "Epoch: 43, Loss: 0.5320837497711182\n",
      "Time taken (seconds): 1.5116298198699951\n",
      "Epoch: 44, Loss: 0.5306951999664307\n",
      "Time taken (seconds): 1.5065598487854004\n",
      "Epoch: 45, Loss: 0.5287045836448669\n",
      "Time taken (seconds): 1.5159962177276611\n",
      "Epoch: 46, Loss: 0.5270751714706421\n",
      "Time taken (seconds): 1.5364220142364502\n",
      "Epoch: 47, Loss: 0.5270388722419739\n",
      "Time taken (seconds): 1.521723747253418\n",
      "Epoch: 48, Loss: 0.5282015800476074\n",
      "Time taken (seconds): 1.5195527076721191\n",
      "Epoch: 49, Loss: 0.5281954407691956\n",
      "Time taken (seconds): 1.5620169639587402\n",
      "Epoch: 50, Loss: 0.5261222720146179\n",
      "Time taken (seconds): 1.5950989723205566\n",
      "Epoch: 51, Loss: 0.5242122411727905\n",
      "Time taken (seconds): 1.5370898246765137\n",
      "Epoch: 52, Loss: 0.5242437720298767\n",
      "Time taken (seconds): 1.5166130065917969\n",
      "Epoch: 53, Loss: 0.5247893333435059\n",
      "Time taken (seconds): 1.5186378955841064\n",
      "Epoch: 54, Loss: 0.5240839123725891\n",
      "Time taken (seconds): 1.5165269374847412\n",
      "Epoch: 55, Loss: 0.5230153799057007\n",
      "Time taken (seconds): 1.5188579559326172\n",
      "Epoch: 56, Loss: 0.5227429270744324\n",
      "Time taken (seconds): 1.5113492012023926\n",
      "Epoch: 57, Loss: 0.522326648235321\n",
      "Time taken (seconds): 1.5227041244506836\n",
      "Epoch: 58, Loss: 0.5212739706039429\n",
      "Time taken (seconds): 1.512972116470337\n",
      "Epoch: 59, Loss: 0.5207734704017639\n",
      "Time taken (seconds): 1.507718801498413\n",
      "Epoch: 60, Loss: 0.5210363864898682\n",
      "Time taken (seconds): 1.5088236331939697\n",
      "Epoch: 61, Loss: 0.5204604864120483\n",
      "Time taken (seconds): 1.516834020614624\n",
      "Epoch: 62, Loss: 0.5195966362953186\n",
      "Time taken (seconds): 1.509376049041748\n",
      "Epoch: 63, Loss: 0.5192992091178894\n",
      "Time taken (seconds): 1.5033419132232666\n",
      "Epoch: 64, Loss: 0.5187334418296814\n",
      "Time taken (seconds): 1.5050930976867676\n",
      "Epoch: 65, Loss: 0.5181053280830383\n",
      "Time taken (seconds): 1.5079057216644287\n",
      "Epoch: 66, Loss: 0.518001914024353\n",
      "Time taken (seconds): 1.5097987651824951\n",
      "Epoch: 67, Loss: 0.5175528526306152\n",
      "Time taken (seconds): 1.5186898708343506\n",
      "Epoch: 68, Loss: 0.5169208645820618\n",
      "Time taken (seconds): 1.5155718326568604\n",
      "Epoch: 69, Loss: 0.5167868733406067\n",
      "Time taken (seconds): 1.517470121383667\n",
      "Epoch: 70, Loss: 0.5162991881370544\n",
      "Time taken (seconds): 1.509883165359497\n",
      "Epoch: 71, Loss: 0.5157628655433655\n",
      "Time taken (seconds): 1.534315824508667\n",
      "Epoch: 72, Loss: 0.5154869556427002\n",
      "Time taken (seconds): 1.5197839736938477\n",
      "Epoch: 73, Loss: 0.5148578882217407\n",
      "Time taken (seconds): 1.5114140510559082\n",
      "Epoch: 74, Loss: 0.5146525502204895\n",
      "Time taken (seconds): 1.5085840225219727\n",
      "Epoch: 75, Loss: 0.5140431523323059\n",
      "Time taken (seconds): 1.5092661380767822\n",
      "Epoch: 76, Loss: 0.5137651562690735\n",
      "Time taken (seconds): 1.506324052810669\n",
      "Epoch: 77, Loss: 0.5132032632827759\n",
      "Time taken (seconds): 1.5077340602874756\n",
      "Epoch: 78, Loss: 0.5129294395446777\n",
      "Time taken (seconds): 1.5109241008758545\n",
      "Epoch: 79, Loss: 0.5122920274734497\n",
      "Time taken (seconds): 1.5069568157196045\n",
      "Epoch: 80, Loss: 0.51201331615448\n",
      "Time taken (seconds): 1.5049431324005127\n",
      "Epoch: 81, Loss: 0.5115464925765991\n",
      "Time taken (seconds): 1.5056169033050537\n",
      "Epoch: 82, Loss: 0.5109766721725464\n",
      "Time taken (seconds): 1.5119352340698242\n",
      "Epoch: 83, Loss: 0.5106640458106995\n",
      "Time taken (seconds): 1.5197539329528809\n",
      "Epoch: 84, Loss: 0.5102177262306213\n",
      "Time taken (seconds): 1.5061759948730469\n",
      "Epoch: 85, Loss: 0.5096348524093628\n",
      "Time taken (seconds): 1.504668951034546\n",
      "Epoch: 86, Loss: 0.5090681314468384\n",
      "Time taken (seconds): 1.5054652690887451\n",
      "Epoch: 87, Loss: 0.508595883846283\n",
      "Time taken (seconds): 1.5016000270843506\n",
      "Epoch: 88, Loss: 0.5080469846725464\n",
      "Time taken (seconds): 1.5075998306274414\n",
      "Epoch: 89, Loss: 0.5073304176330566\n",
      "Time taken (seconds): 1.5108280181884766\n",
      "Epoch: 90, Loss: 0.5066906809806824\n",
      "Time taken (seconds): 1.508768081665039\n",
      "Epoch: 91, Loss: 0.5060160160064697\n",
      "Time taken (seconds): 1.5063929557800293\n",
      "Epoch: 92, Loss: 0.5052482485771179\n",
      "Time taken (seconds): 1.5095748901367188\n",
      "Epoch: 93, Loss: 0.5043965578079224\n",
      "Time taken (seconds): 1.507093906402588\n",
      "Epoch: 94, Loss: 0.5034944415092468\n",
      "Time taken (seconds): 1.504958152770996\n",
      "Epoch: 95, Loss: 0.5026318430900574\n",
      "Time taken (seconds): 1.5034539699554443\n",
      "Epoch: 96, Loss: 0.5017628073692322\n",
      "Time taken (seconds): 1.5173041820526123\n",
      "Epoch: 97, Loss: 0.5008719563484192\n",
      "Time taken (seconds): 1.50297212600708\n",
      "Epoch: 98, Loss: 0.4999663531780243\n",
      "Time taken (seconds): 1.5017831325531006\n",
      "Epoch: 99, Loss: 0.49898913502693176\n",
      "Time taken (seconds): 1.5087199211120605\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "import time\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    # time each epoch\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # pack the data into one tensor\n",
    "    data = torch.cat(tensors_train, dim=0).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Time taken (seconds): {time.time() - start_time}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 400 more epochs\n",
    "\n",
    "for epoch in range(400):\n",
    "    # time each epoch\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # pack the data into one tensor\n",
    "    data = torch.cat(tensors_train, dim=0).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {100 + epoch}, Loss: {loss.item()}\")\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Time taken (seconds): {time.time() - start_time}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
